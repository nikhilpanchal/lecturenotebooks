{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tests\n",
    "\n",
    "Tests are separate bits of code in addition to the main application code that are written to confirm and verify the desired behavior of various scopes of application. These are the responsibility of the `developer` and are typically written as the main application is being written. Each of these tests are **self contained**, in that each test is responsible for setting up the system before running the actual test and tearing it down after the test is run. \n",
    "\n",
    "\n",
    "## Test Types\n",
    "Application tests can be grouped into three main categories, that differ in terms of the scope of the application code that they are written to verify (cover). These are\n",
    "\n",
    " 1. Unit Tests\n",
    " 1. Integration Tests\n",
    " 1. End-to-End (Feature) Tests\n",
    " \n",
    "### Unit Tests\n",
    "\n",
    " - A Unit test has the smallest scope or code coverate of testing where each unit tests checks a small `unit` of code in isolation. \n",
    " - If the unit has any interaction with another dependent unit or module that dependency is `mocked` out.\n",
    " - Is the most fine-grained of all tests, requiring a lot of unit tests to cover the entire codebase.\n",
    " - Each test must execute as quickly as possible in order to lend itself to automation.\n",
    " \n",
    "### Integration Tests\n",
    " \n",
    " - Tests that two or more units or modules work together (integrate) as expected.\n",
    "  - eg. Test that checks for the interaction between server code and a database.\n",
    " - Can take longer to run than unit-tests due to remote calls and other integrations that need to run as part of the test\n",
    " - Requires more setup than unit tests (like setting up a fake server or test database to integrate with)\n",
    " \n",
    "### End-to-End Tests\n",
    "\n",
    " - Also called `Feature Tests`, they test out a high level feature of the application\n",
    "  - Tests the application at the highest level of abstraction, mostly oblivious to the way the app is implemented\n",
    " - These involve testing an application at the browser level, simulating real user interaction with the system\n",
    " - Take the longest to run, most expensive to write and most prone to failure\n",
    "  - Involves communication between the client (browser) and server layers which will take the same time as a real feature execution\n",
    "  - Run using tools like Selenium that has its own learning curve\n",
    "  - Most prone to failure when changes are made to the system, increasing maintenance cost\n",
    "  - Less deterministic than unit or integration tests\n",
    " \n",
    " \n",
    "### Testing Pyramid\n",
    "The rule of thumb for a typical application is to have the largest number of fine-grained unit tests, a smaller number of integration tests and then an even smaller set of end to end tests.\n",
    "\n",
    "![Testing Pyramid](https://2.bp.blogspot.com/-YTzv_O4TnkA/VTgexlumP1I/AAAAAAAAAJ8/57-rnwyvP6g/s1600/image02.png \"Testing Pyramid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tests in Python\n",
    "\n",
    "Testing support in python is mainly provided by the `unittest` module. This provides support for test automation, sharing of test setup and teardown across individual test cases, Grouping of related tests as `Suites` of tests and finally, a report generation framework to print out the results of the test run.\n",
    "\n",
    "`unittest` as the name suggests, is mainly used to write **unit tests**, however they can be used to write **integration tests** as well.\n",
    "\n",
    "### Basic Components\n",
    "\n",
    "In order to write tests for the `unittest` module to run, you would need to provide the following main components in order to be compliant with unittest's automation test runner.\n",
    "\n",
    "#### TestCase\n",
    "A class that encompasses a set of related tests. This class would need to be a `subclass` of the module provided `unittest.TestCase` class.\n",
    "\n",
    "```python\n",
    "import unittest\n",
    "class MyTests(unittest.TestCase):\n",
    "\n",
    "```\n",
    "\n",
    "#### Individual Tests\n",
    "Each individual test is written as an instance function in the TestCase class. In order for these methods to be triggered by the test runner, their names must be of the form **test_***\n",
    "\n",
    "```python\n",
    "class MyTests(unittest.TestCase):\n",
    "    def test_myunit_test(self):\n",
    "        # assertions\n",
    "        \n",
    "```\n",
    "\n",
    "#### Assertion statements\n",
    "The assertion statements are found within each test and is how the expected behavior of the functionality under test is checked. The `unittest.TestCase` class provides a large set of instance `assert*` functions that you can call to make various types of assertions.\n",
    "\n",
    "```python\n",
    "class MyTestCase(unittest.TestCase):\n",
    "    def test_myunit_test(self):\n",
    "        self.assertEqual(1, 1, \"Expected the output to be 1\")\n",
    "```\n",
    "\n",
    "A list of the more commonly used assert statements can be found [here](https://docs.python.org/3.5/library/unittest.html#assert-methods)\n",
    "\n",
    "#### setUp and tearDown\n",
    "Helper methods of the `unittest.TestCase` class that can be overridden to provide setUp and teardown for each test. The setup method is run before the execution of each test and the teardown is run immediately after each test is run. The setup method can also be used to create any kind of shared data used by the tests. This ensures that the TestCase class is `DRY` so that code that is commonly required by all `test_*` methods is written once and re-used.\n",
    "\n",
    "These methods are what makes the `TestCase` class self-contained. Each run of the `TestCase` class must manage its own setup required for the tests to run, and once completed must also be responsible for any clean-up operations required to leave the system in a good state.\n",
    "\n",
    "```python\n",
    "class MyTestCase(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.dep_system = DepSystem()\n",
    "        self.shared_data = list([0, 1, 2, 3])\n",
    "        \n",
    "    def tearDown(self):\n",
    "        self.dep_system.shutDown()\n",
    "```\n",
    "\n",
    "#### TestSuite\n",
    "`unittest.TestSuite` is a class provided to create groups or `suites` of TestCases. \n",
    "\n",
    "```python\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(MyTestCase('test_myunit_test'))\n",
    "    suite.addTest(MyOtherTestCase('test_myother_test'))\n",
    "    return suite\n",
    "```\n",
    "\n",
    "The `unittest` Test runner provides default grouping for tests in each TestCase class for you so unless you want custom grouping into suites, this is less frequently used.\n",
    "\n",
    "#### Decorators\n",
    "There are several test method decorators that you can use to determine the set of tests that you want to run\n",
    "\n",
    " 1. **@unittest.skip** \n",
    "Decorating a `test_*` method with `@unittest.skip` will cause the test runner to skip this test. This test will be marked as skipped in the test run report\n",
    "\n",
    " 1. **@unittest.skipif**\n",
    "Conditionally skip a test\n",
    "\n",
    " 1. **@unittest.expectedFailure**\n",
    "If you know a test is set to fail, you can use this decorator to prevent it messing up your test report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider the following example class that tests out the functionality of the String operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...s.\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.004s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import datetime\n",
    "\n",
    "class StringTests(unittest.TestCase):\n",
    "    \"\"\"Unit testing class for Strings\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        self.str = \"TestString\"\n",
    "\n",
    "    def test_concat_strings(self):\n",
    "        stra = \"Word\"\n",
    "\n",
    "        self.assertEqual(self.str + stra, \"TestStringWord\", \"It should concatenate strings\")\n",
    "\n",
    "    def test_lower(self):\n",
    "        self.assertEqual(self.str.lower(), \"teststring\", \"It should convert all characters to lower case\")\n",
    "        \n",
    "    def test_format(self):\n",
    "        result = \"Format check {}\".format(2)\n",
    "        self.assertEqual(result, \"Format check 2\", \"formatting not done correctly\")\n",
    "        \n",
    "    def test_isdigit(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            self.str.split(2)\n",
    "        \n",
    "    @unittest.skipIf(datetime.date.today().month > 2, \"Can't run this test after Feb\")    \n",
    "    def test_length(self):\n",
    "        result = len(self.str)\n",
    "        self.assertEqual(result, 10, \"String is of length 10\")\n",
    "\n",
    "    def tearDown(self):\n",
    "        pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Arguments to main added due to iPython. You'd run this as unittest.main()\n",
    "    unittest.main(argv=[\"Ignored\"], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*TestCases must be **entirely self-contained units of code** that handle all of the setup and teardown that the individual test assertions require. They should **NOT** rely on any kind of manual intervention. This makes them scriptable and \"automatable\" (TestCases will typically be run as part of an automated build script).*\n",
    "\n",
    "Additionally, the tests should produce minimal output about its inner details. The only output from the tests should be whether the tests all passed and if not which ones failed and why. For failures, it may provide stacktraces to provide points of further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write a simple class called `ArithmeticCalculator` that exposes functions that perform the simple arithmetic operations of `add`, `subtract`, `multiply` and `divide`. Then write a `TestCase` that contains unit tests that test out the functionality of the exposed functions.\n",
    "\n",
    "#### Directory Structure\n",
    "All tests should be written inside a `tests` folder that will be in your `src` folder. The directory structure of the tests folder should mirror that of the src folder.\n",
    "\n",
    "#### Execution Instructions\n",
    "You should unittests Test runner to execute your tests\n",
    "\n",
    " 1. Using the unittest test runner on the command line\n",
    " This will run all of the tests that are in the file `<filename>.py`.\n",
    " ```bash\n",
    " python -m unittest tests/<filename>.py\n",
    " ```\n",
    " \n",
    " 1. Using the discover mechanism\n",
    " This will look for all files under the tests directory with the filename that matches the pattern `*.py` and run them\n",
    " ```bash\n",
    " python -m unittest discover -s tests -p *.py\n",
    " ```\n",
    " \n",
    " 1. Triggering the test runner from within your code\n",
    " ```python\n",
    " if __name__ == \"__main__\":\n",
    "     unittest.main()\n",
    " ```\n",
    " \n",
    "There are several command line options that you can provide to the unittest Test Runner which are described [here](https://docs.python.org/3.5/library/unittest.html#command-line-options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Doubles\n",
    "\n",
    "When writing tests, it is important to ensure the scope of your tests are localized. Especially for unit tests, your tests must focus exclusively on the unit of code under test, with no reliance on any of that unit's dependencies. That's where `test doubles` come in.\n",
    "\n",
    "A **`Test Double`** is code (typically a class or a function) that simplifies and simulates working production code that is a dependency of the unit of code under test. *The term comes from the movie industry where you have `stunt doubles` the replace real actors in dangerous scenes*.\n",
    "\n",
    "eg. Assume the unit under test is a class that makes a call to a database to retrieve data, run some calculations on it and generate a report. In this case, your unit tests should be written to confirm the logic of the calculations and report generation and **NOT** be making actual calls to the database. Database calls will require setup and teardown of actual data which will also cause your tests to take longer to execute. More importantly, you will in all likelihood use a thoroughly tested, in-built library to make the network call to the db, therefore there is no need for your unit tests to test it out as well. Instead, you should make use of a `stub` object that simulates the behavior of a database by returning a fixed set of rows depending on the calculations you want to test. This will make your unit tests much more predictable and quicker to complete.\n",
    "\n",
    "### Types\n",
    "There are five different types of `Test Doubles` that are used in the industry.\n",
    "\n",
    " 1. **Dummy** These are instance objects that never actually get used. \n",
    "  1. They're typically used to fill un-interseting required arguments in a function call\n",
    " 1. **Stub** These are either objects or functions that return a fixed value. \n",
    "  1. Used to confirm behavior of your unit under test when its dependency returns given values.\n",
    " 1. **Spy** A Spy is a Stub, with the added logic to record calls made to it.\n",
    "  1. Used when you want to confirm that a dependency is called `n` number of times and with a certain set of parameters.\n",
    " 1. **Mock** A mock is a Spy setup with pre-defined expectations of interactions and also includes verification logic that checks that expectations are met.\n",
    "  1. An example mock object could be setup to be called `2` times with `x` and `y` as its arguments. When the test is complete you would call is `verify` function to have it ensure that it was indeed called `2` times with the right arguments.\n",
    " 1. **Fake** This is working code used to simulate a larger scope of production code\n",
    "  1. Is used when the real system is not available or usable. eg. When a vendor writes code that uses a firm's proprietary software. Code at the vendor's site has no access to the software and so will make use of a `Fake` to simulate it and test against.\n",
    "  1. Not uncommon for Fakes to have additional unit tests for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Doubles in Python\n",
    "Support for test doubles in python is provided by the unittest module's `mock object library`. \n",
    "\n",
    "### unittest Mocks\n",
    "The main classes are `Mock` and `MagicMock` that allow you to create different kinds of test doubles throughout your code and also provide helper assertion methods that let you monitor how they have been used (eg. how many times they've been called, with which arguments etc.)\n",
    "\n",
    "*MagicMock* is a subclass of Mock with support for mocking python's `magic` methods (`__str__`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call to the dummy stub function, do stub stuff here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest import TestCase\n",
    "from unittest.mock import Mock, MagicMock, patch\n",
    "\n",
    "\n",
    "class Calculation:\n",
    "    crap = \"Some crap\"\n",
    "    \n",
    "    def run(self):\n",
    "        return 10 * 10\n",
    "\n",
    "    def debug_run(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Calculator:\n",
    "    def multiply(self, val, exponent):\n",
    "        result = val\n",
    "\n",
    "        for i in range(1, exponent):\n",
    "            result *= val\n",
    "\n",
    "        return result\n",
    "\n",
    "    def square_and_increment(self, val):\n",
    "        return self.multiply(val, 2) + 1\n",
    "\n",
    "    def invoke(self, calculation):\n",
    "        return calculation.run()\n",
    "\n",
    "    def debug_invoke(self, calculation):\n",
    "        return calculation.debug_run()\n",
    "\n",
    "\n",
    "class MockTestCases(TestCase):\n",
    "    def test_stub(self):\n",
    "        calculator = Calculator()\n",
    "\n",
    "        # Stub out a dependent method since the focus of this test is not multiply.\n",
    "        calculator.multiply = Mock()\n",
    "        calculator.multiply.return_value = 100\n",
    "\n",
    "        result = calculator.square_and_increment(10)\n",
    "        calculator.multiply.assert_called_once_with(10, 2)\n",
    "        self.assertEqual(result, 101, \"Square and increment of 10 should be 101\")\n",
    "        \n",
    "    def test_mock_invocation(self):\n",
    "        # Mocking out an object to ensure its member methods are called as expected\n",
    "        calculation = MagicMock()\n",
    "        calculator = Calculator()\n",
    "        calculator.invoke(calculation)\n",
    "\n",
    "        calculation.run.assert_called_with()\n",
    "\n",
    "    def test_return_value(self):\n",
    "        mock = Mock()\n",
    "        mock.sample_field = 30\n",
    "        mock.return_value = 10\n",
    "\n",
    "        mock.sample_method.return_value = 20\n",
    "\n",
    "        self.assertEqual(mock(), 10)\n",
    "        self.assertEqual(mock.sample_field, 30)\n",
    "        self.assertEqual(mock.sample_method(), 20)\n",
    "\n",
    "    # Side effects.\n",
    "    def test_side_effects(self):\n",
    "        # Setup a stub to throw an exection\n",
    "        mock = Mock()\n",
    "        mock.debug_run.side_effect = Exception(\"Erroneous call\")\n",
    "\n",
    "        calculator = Calculator()\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            calculator.debug_invoke(mock)\n",
    "\n",
    "    def test_stub_function(self):\n",
    "        # Or have the mock call another canned function\n",
    "        def canned_func():\n",
    "            print(\"Call to the dummy stub function, do stub stuff here\")\n",
    "            return 500\n",
    "\n",
    "        mock = Mock()\n",
    "        mock.debug_run.side_effect = canned_func\n",
    "        mock.crap.return_value = 100\n",
    "\n",
    "        calculator = Calculator()\n",
    "        self.assertEqual(calculator.debug_invoke(mock), 500)\n",
    "\n",
    "    # Creating a mock based on a class definition\n",
    "    def test_mock_spec(self):\n",
    "        mock = MagicMock(spec=Calculation)\n",
    "        mock.debug_run.return_value = 10\n",
    "        \n",
    "        # Spec'd mock should throw an error here because Calculation has no attribute called crap.\n",
    "        mock.crap.return_value = 100\n",
    "\n",
    "        calculator = Calculator()\n",
    "        calculator.debug_invoke(mock)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Arguments to main added due to iPython. You'd run this as unittest.main()\n",
    "    unittest.main(argv=[\"Ignored\"], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Patch: Use of Mocks in Test Cases\n",
    "\n",
    "The simplest way to create mocks is with the @patch decorator. The specified class or object will be replaced with instances of `MagicMock` and passed as arguments to the decorated function.\n",
    "\n",
    "```python\n",
    "import unitest.mock.patch\n",
    "\n",
    "@patch(\"othermodule.OtherClass\")\n",
    "def test_some_method(self, otherClassMock):\n",
    "        otherClassMock.other_function.return_value = 10\n",
    "        \n",
    "        result = otherClassMock.other_function()\n",
    "        \n",
    "        self.assertEqual(request, 10)\n",
    "```\n",
    "\n",
    "Remember, the use of mocks in a unit test case is to setup test doubles for the `dependencies` of the unit of code under test, so that your tests can focus on it's functionality rather than test out anything in the dependency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider the following class that exposes a function to delete a file. It first checks to see whether the file exists and if so delete it, and it uses the in-built os library to do so.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class RemovalService(object):\n",
    "    \"\"\"Sample class that wraps the rm os call\"\"\"\n",
    "\n",
    "    def rm(self, file_name):\n",
    "        if os.path.isfile(file_name):\n",
    "            os.remove(file_name)\n",
    "```     \n",
    "\n",
    "\n",
    "The Unit TestCase class to for the RemovalService class should have tests that confirm the logic decision of RemovalService, **NOT** the in-built os library. i.e. Your tests should **NOT** actually delete a file off the system. You accomplish this by providing mocks for the in-built `os` and `os.patch` modules.\n",
    "\n",
    "\n",
    "```python\n",
    "from mymodule import RemovalService\n",
    "\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    "\n",
    "class RemovalServiceTestCase(TestCase)\n",
    "    @patch('mymodule.os.path')\n",
    "    @patch('mymodule.os')\n",
    "    def test_rm_on_a_nonexistent_file(self, mock_os, mock_path):\n",
    "        \"\"\"Testing the rm method on a non-existent file\"\"\"\n",
    "        \n",
    "        # Setup the mock to simulate a file that doesn't exist\n",
    "        mock_path.isfile.return_value = False\n",
    "\n",
    "        service = RemovalService()\n",
    "        service.rm(\"my_file\") # Doesn't need to be the path to an actual file since we're using mocks.\n",
    "\n",
    "        self.assertFalse(mock_os.remove.called, \"The remove call should not go through on a non-existent file\")\n",
    "\n",
    "    @patch('mymodule.os.path')\n",
    "    @patch('mymodule.os')\n",
    "    def test_rm_on_an_existing_file(self, mock_os, mock_path):\n",
    "        \"\"\"Testing the rm method for a file that does exist\"\"\"\n",
    "        mock_path.isfile.return_value = True\n",
    "\n",
    "        service = RemovalService()\n",
    "        service.rm(\"my_file\")\n",
    "\n",
    "        # Check that the call went through with the right arguments\n",
    "        mock_os.remove.assert_called_with(\"my_file\")\n",
    "```\n",
    "\n",
    "Points to note\n",
    "\n",
    " * The mocks that you create using `@patch` should on the actual classes that Class under test imports. Note how the argument to patch is `mymodule.os.path` and not just `os.path`. You want the instance of `os` that got imported in `RemovalService`\n",
    " * Mocks inserted using nested `@patch` are done in reverse order. `mymodule.os` is the closest decorator to the `test_*` functions so its mock is the first parameter to those functions. `mymodule.os.patch` is the outer decorator so it goes as the second argument\n",
    " * The mocks are created on the **dependencies** of `RemovalService`. Since we want to test out `RemovalService` we mock out the other dependent classes of functions that it calls, in order to keep our test focus only on the `RemovalService` logic.\n",
    " * This TestCase is entirely self-contained. Through the use of mocks it does not depend on any file to exist or require any actual file to be deleted, and thus can be run on any machine with any setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Driven Development (TDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Test Driven Development, relates to writing code as per specifications set by tests. The process involvles capturing user requirements, or system behaviour in small incremental tests, and then writing code to get those tests to pass. This typically results in building systems incrementally with small blocks of code and then building additional behavior that sits on top of previously built code, taking one test at a time and getting it to pass.\n",
    "\n",
    "The process follows three main steps that are then repeated over and over again.\n",
    "\n",
    " - Red\n",
    "  - Write a test before the code (causing it to fail). The missing code causes the test to go `Red`.\n",
    " - Green\n",
    "  - Write the code to get the test to pass using the `Quickest path to Green` method.\n",
    " - Refactor\n",
    "  - Refactor the code to better structure it whilst keeping the tests green. The Green tests ensure you are starting this test with working code.\n",
    "\n",
    "Repeat with the next required behaviour.\n",
    "\n",
    "### Advantages\n",
    " - Incremental build of the system allows its architecture to `evolve` as per the systems need.\n",
    "  - As opposed to deciding an architecture up front which would most likely not capture all cases.\n",
    "  - Results in a system that is more amenable to change.\n",
    " - Results in (close to) 100% test coverage, ensuring well tested code that inspires developer and user confidence\n",
    " - The tests ensure new features can be added or existing code can be modified without breaking functionality\n",
    " - Bugs or logic breaks are caught and fixed early in development process\n",
    "  - Any code change that breaks existing logic will create red tests, leading to early bug detection and fixes\n",
    " - Results in the testing cycle being almost entirely automated with tests run dozens of times a day or on every build\n",
    "  - *Important to keep tests small and their executions quick*\n",
    "  \n",
    "### Concerns\n",
    " - Can *appear* to take longer time due to `double` work\n",
    " - Counter-intuitive: Not natural to first timers to write a test before writing the real code\n",
    " - Can result in `brittle` tests: Any small change in the code result in the refactoring of the tests as well.\n",
    "  - Usually occurs if the tests are written after the code\n",
    "  - *This is why tests should check for `behaviour` not code*.\n",
    " - Different languages provide differing levels of support\n",
    " \n",
    "In the real world BDD is implemented in various differing flavors. Teams often write the tests after writing code to have tests serve as safeguards against logic-breaking code modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Use the concepts you have learned in this lesson to write unit tests for the option payoff calculation project. Tests much be written for each of the classes that check the logic of that class under test, with all of the dependencies of that class mocked out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Resources\n",
    "\n",
    "[unittests in the Python standard library](https://docs.python.org/3.5/library/unittest.html)  \n",
    "[unittest.mock](https://docs.python.org/3.5/library/unittest.mock.html)  \n",
    "[unittest.mock examples](https://docs.python.org/3.5/library/unittest.mock-examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}